{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_onlyCode(allLines):\n",
    "    copyline = allLines.copy()\n",
    "    newLines=[]\n",
    "    lines=iter(allLines)\n",
    "    citer=0\n",
    "    withoutSpace = lambda x : x.replace(' ','')\n",
    "    blankLine = lambda x : x=='\\n' or x==''\n",
    "    commWildcard = lambda x:x[0]=='#'\n",
    "    commOneline = lambda x:(initComment(x) and endComment(x[3:]))\n",
    "    initComment = lambda x:(x[:3]=='\"\"\"')\n",
    "    endComment = lambda x:(x[-4:-1]=='\"\"\"')\n",
    "    for line in lines:\n",
    "        citer=citer+1\n",
    "        newLine = withoutSpace(line)\n",
    "        if blankLine(newLine):\n",
    "            continue\n",
    "        if commWildcard(newLine):\n",
    "            continue\n",
    "        if commOneline(newLine):\n",
    "            continue\n",
    "        if initComment(newLine):\n",
    "            try:\n",
    "                line = next(lines)\n",
    "            except:\n",
    "                pass\n",
    "            for i in range(citer,len(copyline)):\n",
    "                newLine = withoutSpace(line)\n",
    "                endLine = citer+1==len(copyline)\n",
    "                try:\n",
    "                    line = next(lines)\n",
    "                except:\n",
    "                    pass\n",
    "                if endComment(newLine) or endLine:\n",
    "                    break\n",
    "                else:\n",
    "                    citer=citer+1\n",
    "        newLines.append(line)   \n",
    "    return newLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depFor(archivo):\n",
    "    newLineas=archivo.copy()\n",
    "    lineas=iter(archivo)\n",
    "    citer=0\n",
    "    withoutSpace = lambda x : x.replace(' ','')\n",
    "    initFor = lambda x:(x[:3]=='for')\n",
    "    endFor = lambda x:(x[-2:-1]==':')\n",
    "    countSpace = lambda x:(x.index(x.replace(' ','')[0]))\n",
    "    arrMax=[0]\n",
    "    arr=[]\n",
    "    for linea in lineas:\n",
    "        citer=citer+1\n",
    "        wordFor='for'\n",
    "        wordEnd=':'\n",
    "        space = countSpace(linea)\n",
    "        patronFor=((initFor(withoutSpace(linea))) and (endFor(withoutSpace(linea))))\n",
    "        if patronFor:\n",
    "            newLines=[]\n",
    "            for i in range(citer,len(newLineas)):\n",
    "                endLine = citer+1==len(newLineas)\n",
    "                forSpace = False\n",
    "                if not endLine:\n",
    "                    nextspaceLine = countSpace(newLineas[citer])\n",
    "                    forSpace = nextspaceLine<=space\n",
    "                else:\n",
    "                    try:\n",
    "                        newLines.append(next(lineas))\n",
    "                    except:\n",
    "                        pass\n",
    "                if forSpace or endLine:\n",
    "                    value,arr=depFor(newLines)\n",
    "                    arrMax.append(value+1)\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        newLines.append(next(lineas))\n",
    "                        citer=citer+1\n",
    "                    except:\n",
    "                        pass\n",
    "    return(max(arrMax),arrMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def librerias(lines):\n",
    "    setlibrerias=set()\n",
    "    wordImport= 'import'\n",
    "    wordFrom= 'from'\n",
    "    notElement={'import', 'from', ''}\n",
    "    for line in lines:\n",
    "        inImport=(wordImport in line)\n",
    "        inFrom=(wordFrom in line)\n",
    "        if inFrom and inImport:\n",
    "            arrlibr = line[:line.index('import')].split(' ')\n",
    "            setlibrerias = setlibrerias|set(arrlibr)\n",
    "            continue\n",
    "        if inImport:\n",
    "            arrlibr =  line[:-1].split(' ')\n",
    "            setlibrerias = setlibrerias|set(arrlibr)\n",
    "            continue\n",
    "    newSetelement= list(setlibrerias-notElement)\n",
    "    newArrelement = [ element.split('.')[0] for element in newSetelement]\n",
    "    return set(newArrelement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDef(lines):\n",
    "    wordDef='def'\n",
    "    wordEnd=':'\n",
    "    arrArg=[]\n",
    "    for line in lines:\n",
    "        if wordDef in line and wordEnd in line:\n",
    "            argini=line.find(\"(\")+1\n",
    "            argend=line.find(\")\")\n",
    "            arg=line[argini:argend].split(',')\n",
    "            arrArg.append(len(arg))\n",
    "    return arrArg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicateLine(arrlines):\n",
    "    newGroup=[]\n",
    "    withoutSpace = lambda x : x.replace(' ','')\n",
    "    lines=list(map(withoutSpace,arrlines))\n",
    "    unionLine=lambda x : x.replace('[','').replace(',','').replace(']','')\n",
    "    for i,line in enumerate(lines):\n",
    "        newGroup.append(str(unionLine(str(lines[i:i+4]))))\n",
    "    repetidos=collections.Counter(newGroup)\n",
    "    duplicado = [1 for key in repetidos if repetidos[key]>=2]\n",
    "    return (sum(duplicado)+3*bool(duplicado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnFilespy(carpeta):\n",
    "    newArray=[]\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        if os.path.isdir(os.path.join(carpeta,archivo)):\n",
    "            newArray = newArray+returnFilespy(os.path.join(carpeta,archivo))\n",
    "        else:\n",
    "            newArray.append(os.path.join(carpeta,archivo))\n",
    "    filesPy = [filePy for filePy in newArray if filePy[-3:]=='.py' ]\n",
    "    return filesPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3411"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruoteFlies = '.\\\\Almacen_repositorios\\\\tensorflow-master'\n",
    "filesPy = returnFilespy(ruoteFlies)\n",
    "len(filesPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________\n",
      "\n",
      "number of lines : 696194\n",
      "libraries : ['', 'disable=protected-access', ':)', 'multiprocessing', 'input_map,', '1)', 'na_values=\"?\")', 'second_imported', 'get_element_from_tensor_info(tensor_info,', 'tag=\"feature_importance/usage_fraction\",', 'keras', '3]),', 'target', 'expected_shape=expected_shape,', 'imported_vars,', '!=', 'report_feature_importances:', 'imported_concrete,', \"'does\", 'format_import(source_module_name,', '(', 'wrapped_import', 'from_control_flow_context_def(nested_def,', '16', 'subprocess', 'e_x', 'feature_importances[\"feature_d\"],', 'simple_value=usage_count)', 'var_list', 'v', 'saver_lib', 'ct', 'tensorboard', '_os_path', 'queue_runner_def=None,', 'import_scope=to_scope)', 'getpass', 'critical_section_def,', 'imported', 'tag=\"feature_importance/gains_fraction\",', 'zipfile', '_logging', 'proto', 'values', 'warnings', 'import_scope)', 'set,', 'newgraph', '__future__', '_inspect', '+', '[])', 'import_event(tensor,', 'clear_devices=False,', 'pipes', 'name,', 'IMPORTS', 'test_getfutureimports_functions(self):', 'E', \"'If\", 'b', 'requires', '`Tensor`', 'soundfile', 'portpicker', 'ops', 'orig_meta_graph,', 'key=lambda', 'Failed', 'glob', 'error', '_site', 'since', 'defg=7', 'output_name', 'sorted(imports)[0]', 'module_code_builder', 'imports85', 'expectation_importance_sampler(f,', 'raise', '{', 'ctypes', 'inspect', '%s:', 'original_graph_def,', '_test_import(include_collection_keys,', 'saver,', \"'from\", '_,', 'print(sys', 'True', \"'import\", 'cycles=1,', \"'Model')\", \"r'^from\", '%s\"', \"'problem\", \"('absolute_import',\", 'import_str', \"'python/platform/control_imports\", 'will', 'distutils', 'result', '[[3', 'import_scope,', '_sys', 'constant_op', 'cpuinfo', '_uuid', 'dst_scope=\"imported\")', 'as', 'grpc', '\"moment', 'c', '\\'\"#API', 'in', 'gen_summary_ops', 'classifier_outputs', 'copy', 'saver_module', 'we', \"\\\\'import/y\\\\'\", 'Conc(1,', 'here', 'yaml', 'binascii', 'deal', 'return_elements=[7])', 'time', '{}))', '**saver_kwargs):', 'imports,', 'graph_builder', 'termcolor', 'alias', 'gast', 'used', 'import_scope=\"importA\")', 'gc', '_get_imports85():', 'signal', 'dd', \"msg='%s\", 'RuntimeError(\"Exporting/importing', 'import_scope=None):', 'export_dir,', 'of', 'you\")', 'tf_logging', 'traceback', 'import_scope=None,', 'get_init_op(meta_graph_def,', 'open(self', '_compute_gradient(imported', 'tempfile', 'graph=g,', 'future_imports', 'cycles)', 'ssl', 'pylint:', 'import_scoped_meta_graph_with_return_elements(', 'sample_var', 'clear_devices=True,', '\"beta\"]}))', 'restorer', \"'\", 'test)', 'ast', 'cgi', 'PLACEHOLDER\"', 'imported_modules', 'test_load_with_import_scope(self,', '\"b\":', 'name=import_scope)', 'scope', 'new_saver_3', 'feature_names,', '\"w\")', 'third_path,', '__builtin__', 'be', 'import', 'necessary', 'frozenset(important_op_names)', 'concurrent', '-1],', 'submodules', '\"y\":', \"feature_importances']\", '[b\"loc:@imported_graph/A\"])', 'sf', 'feature_importances[\"feature_a_0\"],', '`input_map`', 'try_import(name):', 'net', 'import_scope=import_scope,', 'v)', 'uuid', 'api_name):', 'containing', '_copy', '[\"foo_graph\"],', 'FUTURES_PATTERN', 'math', \"test_op')\", 'sparsemax_loss', 'variadic', '_distutils', 'second_path,', '_traceback', 'op_hint', 'tensorflow_estimator', 'test_dir', \"@tf_export('graph_util\", 'kubernetes', \"_TEST_CONSTANT')\", 'python_types', 'new', 'import_and_eval(flags_obj):', \"name='import')\", 'pickle', 'requests', 'custom_regression', 'modules', 'from_proto(proto,', 'ResourceVariable', '1', 'the', 'report_feature_importances=False,', 'importlib', 'import_scope=import_scope))', 'replaced', 'stats', 'future_features', 'prepend_name_scope(name,', 'import_to_tensorboard(FLAGS', 'import_meta_graph(meta_graph_or_file,', 'a', 'places=4)', 'dst_graph=copied_graph,', 'WhileContext(context_def=context_def,', 'e_x2', 'tensorflow_probability', 'values_def=context_def', \"pydot'\", '==', 'expected_', 'num_dense_floats,', 'import_scope=scope_to_prepend_to_names)', '[\"gamma\",', 'builder_cls):', 'feature_importances[\"feature_a_m_3\"],', 'importance', 'fnmatch', 'please', '_json', 'feature_importances[\"feature_b\"],', 'log_dir):', 'print(\"****', 'export_scope=\"import\")', 'keras_preprocessing', '{\"key1\":', 'a,', '{\"output_0\":', 'absolute_import', 'with', 'difflib', '(expected_import,', '_contextlib', 'free', '_error', 'gym', 'str(e)))', 'keras_applications', 'new_saver_1', 'wrap_function(_imports_graph_def,', 'imported(constant_op', 'import_scope=\"new_queue1\")', '(expected,', '[imported', 'inputs),', 'tf_inspect', 'sampling', 'If', 'equal', 'tflite_runtime', \"%s'\", 'feature_importances', 'install', 'if', 'Unused', 'accidental', 'shlex', 'types_lib', 'signatures={})', 'str(imports', 'tf_export', '\"some_scope_name\"', 'has', 'second_import', 'hook', 'cycles):', 'imported(NamedTupleType(a=constant_op', '\"import', 'str(imports),', '_import_and_infer(save_dir,', '%f\"', 'cerberus', 'input_x,', 'disable=,wildcard-import,unused-import', 'tensor_name', 'import_scope=\"new_hidden1\")', 'graph=graph,', 'string', 'pydotplus', 'dest_name_to_imports', 'concrete', 'inconsistent', 'disable=unused-import', 'context_def', 'This', '=', 'measures', 'tf\\\\na', 'values_def=c', 'functools', '_import_meta_graph(self):', '[0', 'no', 'return_elements=[\"B\"],', '\"meta_graph', '@tf_export(v1=[\"train', 'builtins', 'input_name', 'Queue', 'test_getfutureimports_lambdas(self):', 'get_train_op(meta_graph_def,', \"''\", 'mock', 'var_v1', 'estimate', 'import_scope=\"import\")', 'binary', 'importing', \"\\\\'import/b\\\\'\", '%s', 'oauth2client', 'from_proto(hparam_def,', 'output_op', 'graph,', 'further', 'var_list,', 'a_1,', \"import_scope='new_outer')\", 'weakref', 'root', '\"tf', 'test_ref_variable_import(self):', 'values_def=None,', 'library:', 'list(imported', 'typing', 'imported_output)', 'circular', '_importer', '(source_name,', 'ImportError(\"Could', '_from_proto_fn(v,', 'idx,', 'constants', '(name,', 'queue_runner_def,', 'len(imported', 'None)', 'signatures=second_import', 'from_proto(variable_def,', 'expected', 'boto3', '_time', '\"\"\"from', 's:', 'sorted_by_importance', 'context_def,', \"'import',\", 'CondContext(context_def=context_def,', 'and', 'dtype=imports85', 're;', 'text,', '{\"x\":', 'math_ops;', 'calling', \"'expectation_importance_sampler_logspace',\", 'python', 'Summary', '\"Feel', 'types', 'before', 'output_op_name', 'load_graph(self,', 'dest_name)', 'import_scope=\"s\")', 'data', 'values_def,', 'import_scope=\"subgraph_2\")', 'node', 'import_scope[-1]', 'imported_path', \"ImportError('Failed\", 'fractions', '_get_feature_importances(dtec,', 'json', 'random', 'output_layer', 'astor', '_tempfile', '360', 'import_scoped_meta_graph(orig_meta_graph,', 'dependency', 'disable=g-import-not-at-top,unused-import', 'load(sess,', 'full_api_name)', 'wrapt', 'disable=invalid-name', 'ValueError(\"Exporting/importing', 'disable=g-bad-import-order', 'exist', '_numbers', 'train', 'errno', 'r\"/\\\\2\",', 'imported({\"a\":', 'input_y', '_TestDir(\"scoped_export_import\")', 'test_resource_variable_import(self):', \"'-c',\", 'textwrap', '_create_saver_from_', '_six', 'imported_return_elements', 'c,', 'input_layer', '\"Operation', 'enumerate(self', '3]))', 'comment', '5))}))', 'base_tensor_name', '`Log[f]`', '_import_config(self):', 'pylint:disable=g-import-not-at-top', '_get_op_from_signature_def(meta_graph_def,', 'output,', 'return', 'tiled_imported,', '`name`', 'list(tape', '\"list', '\"r\")', 'Please', 'curses', 'prob', 'import_scope=\"new_model\")', 'imported_concrete', 'issue', '[op', 'Do', 'from_proto(saver_def,', 'tokenize', 'import_scope', 'imported_graph', '\"', 'itertools', \"msg='compat\", 'import_scope))', 'collections_lib', 'get_asset_tensors(export_dir,', 'nest', 'an', \"'division',\", 'pd', \"name='expectation_importance_sampler'):\", 'attr', 'load(self,', 'epochs=3)', 'it', 'simple_value=gain)', \"print('Failed\", '[\"tag_name\"],', 'queue_runner_def', 'toolz', 'mpi', 'URL', 'Necessary', 'tiled_imported', '_collections', 'new_var_list', 'supplement', 'save', 'imported_constant', '\"alpha\"]}))', 'dest_module,', 'output2', 'struct', 'import_scope):', '_imports_graph_def():', 'report_feature_importances=report_feature_importances,', 'trained', 'numbers', 'sgdr_learning_rate_decay', 'can', 'test_getfutureimports_methods(self):', 'variable_def', 'loader', 'NotImplementedError(\"variable_scope', 'new_saver_2', 'd', '4', 'tag=\"feature_importance/usage_counts\",', 'b,', 'expected_import', 'threading', 'dest_name):', 'lengths', '[]', 'imported_vars)', 'pydot_ng', 'must', 'source_name', 'tape', 'There', '0,', 'disable=wildcard-import', 'mc', 'import_scope=\"my_scope\")', 'meta', 'API', 'z3', 'npr', 'cPickle', 'name=\"final\")', 'import_scope[:-1]', 'distribute_strategy=distribute_strategy,', 'applied', 'os', 'item', '[b\"loc:@imported_graph/B\"])', 'False', 'for', 'proto,', '_init_from_proto(self,', 'h5py', '_get_feature_importances(', 'Avoids', 'into', 'sess', 'tensor_name)', 'sorted_by_importance))', '2', 'name=\"imported_graph\")', 'b\"loc:@imported_graph/B\"])', 't', '[b\"loc:@imported_graph/A\",', 'avoid', 'imported_modules:', 'new_saver', 'load', 'text', '_platform', 'imported_variables', 'str(imports)))', 'zlib', 'linecache', 'special', 'linear_regression_categorical', 'imported_output', 'test_structure_import(self,', 'sqlite3', 'import_scope=\"new_hidden1\",', 'scipy', 'meta_graph_def_to_load,', '@test', 'child_pkg', '(import_scope,', '3,', 'getfutureimports(entity):', 'output1', \"ops_at_end['1:\", 'run_shell([python_bin_path,', 'restore_variables(self,', 'meta_graph_or_file,', 'important_op_names:', 'imported_vars[sample_key]', 'numpy', \"'TestClass'\", 'hashlib', '2],', 'create_python_api', 'this', 'REQUIRED_FUTURES', 'pdb', '{}', 'tarfile', '_original_from_proto(v,', 'f_concrete', '(api_name', 'set(imported', 'googleapiclient', 'FEATURE_IMPORTANCE_NAME', 'csv', 'kwargs', 'psutil_import_succeeded', 'atexit', 'names', 'input_map', 'imported_constant,', 'sps', 'None)})', 'pprint', 'timeit', 'loss', 'graph', 'var_names', 'psutil_import_succeeded:', 'feature', 'symbol_id', 'from_control_flow_context_def(context_def,', 'tf;\\\\', '_import_graph_def(graph_def,', 'sess,', 'sys;', 'name=\"\")', 'import_scope=\"baz\")', 'packages', '-c', '{model', '-x[1])', 'nets', 'Fs(20),', \"'expectation_importance_sampler',\", 'sklearn', 'fftpack', 'import_scope=\"bar\")', 'add_imports_for_symbol(', 'g_concrete', '**saver_kwargs)', 'colorsys', 'compat', 'saver_for_restore', 'SavedModel', 'variable_def=variable_def,', '\"import/\"', 'True)', '\"b\"]),', 'tfp', 'test_op', 'init_op', 'learning_rate=0', 'setattr', 'run', 'site', 'app', 'tf_saver', 'marshal', 'module', '_math', 'parent_module,', 'custom_export_strategy', 'k', 'The', '-1,', '#', 'new_value', 'you,', 'args', 'gen_lookup_ops', 'import_scoped_meta_graph(meta_graph_or_file,', 'open(imported_output,', 'tf', 'expressions', 'import:', 'None', 'critical_section_def=None,', '\"/\":', 'mark_as_used=False)', 'Unable', 'by', 'io', 'signatures=imported', 'important_op_names', 'readline', 'supported', 'scope=import_scope)', 'PIL', 'code', 'You', '_TestDir(\"selected_collections_import\"),', '\"v:0\")', 'during', 'import_scope=\"\")', 'imp', 'graph=graph2)', 'print(\"\\\\\\\\n\"', 'f:', 'epochs=4)', 'k,', 'third_party', 'restored_fullargspec', '_get_imports85()', 'variable_def,', 'parent_module', 'import_scope=\"subgraph_1\")', 'shutil', 'graph=None,', 'key,', '_re', 'urllib2', 'memory_profiler', 'FLAGS', 'first_imported', 'import_graph_def(graph_def,', 'cStringIO', 'that', 'import_scope:', 'sre_constants', '`expectation_importance_sampler_logspace`', 'base64', 'import_scope=\"scope_name\")', '1,', 'to', '_import_and_infer(', 'name=None):', 'op_signature_key,', '[2', 'outputs))', '_check_already_imported(self,', '(import_node,', 'ret', 'set(self', 'name', 'conversion', 'from_proto(context_def,', 'datetime', 'import_scope=\"baa\")', \"'loc:@imported_graph/A'\", \"help='Base\", 'when', 'inputs):', 'fcntl', 'module_text_map[dest_module]', 'Need', 'logging', 'output', 'spec', \"'\\\\n'\", '[', \"'tf\", 'tensorflow', 'single_image_random_dot_stereograms', 'imported_graph:', 'codecs', 'import_scope=\"new_softmax_linear\",', 'try_import(\"scipy', \"test_op1')\", 'graph=graph_2,', 'gzip', '_', '\"https://archive', \"'`input_map`\", '{})[\"output_0\"]', 'tags,', 'excess', 'b_1', 'do', '`shape`', 'def', 'scope:', 'saver', '}\",', 're', 'sorted(', 'import_scope=\"new_hidden1/new_hidden2\",', 'len(self', 'plt', 'was', 'names=imports85', 'fetches', 'add_import(', 'disable=unused-import,g-bad-import-order,g-import-not-at-top,wildcard-import', 'out', 'future_import_module_statements', 'variables', '%', '{\"_get_imports85\":', 'TensorBoard', 'package', '7', 'imports', 'pycoll', 'operator', '_portpicker_import_error:', 'tensorflow_docs', 'imported_tensor', 'google', 'load_op_from_signature_def(signature_def,', 'pydot', 'clear_devices=True)', 'np', '(lambda:', '_init_values_from_proto(self,', \"'global_feature_importance'\", 'disable=g-import-not-at-top,unused-variable', 'absl', 'import_scope=\"new_hidden2\",', 'del', 'meta_graph', 'run_init_ops(self,', 'matplotlib', 'argparse', 'summary_ops_v2', 'part', 'graphs', 'op', 'scipy_sparse', 'sys', 'collections', 'filename', 'abc', '\"%s/%s\"', 'import_scope=import_scope)', 'log_e_x2', 'function', '\"important', '{\"keys\":', 'omit_collection_keys):', 'x', '(train,', 'r\"\\\\1\"', 'named', 'docker', 'import_scope=\"foo\")', 'tensor_info,', \"name='')\", 'imported_function(two)[\"output_0\"]', 'save_slice_info_def', 'import_to_tensorboard(model_dir,', 'pasta', \"TestClass')\", 'make', '\\\\\\'_tpu_replicate\\\\\\'\"):', 'not', 'setuptools', 'py_collections', 'g', '_import_and_infer(second_dir,', 'dask', 'summary', 'print_function', 'FUTURES_PATTERN_3', 'logits_out', 'op_index,', 'source_name,', '_portpicker_import_error', 'list(imported_vars', '\"Feature', \"set(['absolute_import',\", 'meta_graph_def,', 'Tensorflow', '_np', 'Saver(saver_def=saver_def,', 'self', 'stat', 'inp,', 'imported_function', 'class_node),', 'because', 'public', 'inspect_utils', 'disable=raising-bad-type', 'math_ops', '[loss()', 'importer', '*', 'symbol_id,', 'variable_scope', 'Fs(10))\"', 'array_ops', '_import_submodules(self):', 'or', '_os', 'queue', 'imported_function(x=two)[\"output_0\"]', 'pandas', \"'import_graph_def')\", \"'print_function'])\", 'tfe', 'enum', 'unittest', 'path', 'parent_pkg', 'IPython', 'disable=g-import-not-at-top', '\\\\n\")', 'feature_importances(self):', 'x:', 'non-empty', 'imported_vars):', 'becoming', 'import_graph', 'import_scope=\"test_scope\")', 'psutil', '4]))', 'dnn_regression', '_import_and_infer(export_dir,', 'important', 't,', 'socket', 'from_proto(queue_runner_def,', '\"\",', '2,', 'six', '_test_import(', 'topo_order', 'needs', 'contextlib', 'clear_devices=False)', 'cycles,', 'import_scope=scope_to_prepend_to_names))', 'get_tensor_from_tensor_info(tensor_info,', 'urllib', \"frozenset(['absolute_import',\", '3', 'TF-TRT', 'clear_devices,', 'platform', 'third_import', 'is', 'grad', 'site;', '(k,', 'their', 'imports_list', 'name=(import_scope', 'scope_to_prepend_to_names),', 'have', 'file', 'fn', '__init__(self,', '**kwargs):', 'imported\")', 'f', 'sure', 'run_context', 'future_import_module_statements)', '_subprocess']\n",
      "nesting factor : 1.2306426976853484\n",
      "code duplication : 5.609643289083215\n",
      "average variables : 1.5371785700839578\n",
      "84.24241185188293\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "LinesCodetotal = 0\n",
    "depForTotal = []\n",
    "argDeftotal = []\n",
    "libreryTotal =set()\n",
    "duplicateTotal = 0\n",
    "for filePy in filesPy:\n",
    "#     print('********************************************************')\n",
    "#     print(filePy)\n",
    "    file = open(filePy, 'r', encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    linesCode = def_onlyCode(lines)\n",
    "    LinesCodetotal=LinesCodetotal+len(linesCode)\n",
    "#     print(len(linesCode))\n",
    "    \n",
    "    countFor = depFor(linesCode)[1][1:]\n",
    "    depForTotal = depForTotal+countFor\n",
    "#     print(countFor)\n",
    "    \n",
    "    libreriasSet = librerias(linesCode)\n",
    "    libreryTotal = libreryTotal|libreriasSet\n",
    "#     print(libreriasSet)\n",
    "    \n",
    "    countArgdef = countDef(linesCode)\n",
    "    argDeftotal = argDeftotal+countArgdef\n",
    "#     print(countArgdef)\n",
    "    \n",
    "    isDupplicate = duplicateLine(linesCode)\n",
    "    duplicateTotal=duplicateTotal+isDupplicate\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "avgFor=np.average(depForTotal)\n",
    "perctDuplicate = np.array(duplicateTotal)/np.array(LinesCodetotal)*100\n",
    "avgArg = np.average(argDeftotal)\n",
    "\n",
    "print('____________________________________________________')\n",
    "print()\n",
    "print('number of lines : ' +str(LinesCodetotal))\n",
    "print('libraries : '+str(list(libreryTotal)))\n",
    "print('nesting factor : '+str(avgFor))\n",
    "print('code duplication : '+str(perctDuplicate))\n",
    "print('average variables : '+str(avgArg))\n",
    "\n",
    "elapsed_time = time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047\n",
      "[1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
      "{\"'-c',\", 'os', 'sys;', 'run_shell([python_bin_path,', '__future__', 're', 'argparse', 'distutils', 'platform', \"'import\", 'print(sys', 'site;', \"'from\", 'print(\"\\\\\\\\n\"', 'subprocess', 'shutil', 'errno', 'sys', 'return'}\n",
      "[1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n",
      "8\n",
      "0.015962600708007812\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "# for filePy in filesPy\n",
    "#open file\n",
    "file = open(r'.\\Almacen_repositorios\\tensorflow-master\\configure.py', 'r')\n",
    "lines = file.readlines()\n",
    "\n",
    "##Prosesing file\n",
    "\n",
    "#Lines of code\n",
    "linesCode = def_onlyCode(lines)\n",
    "# for line in linesCode:\n",
    "#     print(line)\n",
    "print(len(linesCode))\n",
    "\n",
    "countFor = depFor(linesCode)\n",
    "print(countFor[1][1:])\n",
    "\n",
    "libreriasSet = librerias(linesCode)\n",
    "print(libreriasSet)\n",
    "\n",
    "countArgdef = countDef(linesCode)\n",
    "print(countArgdef)\n",
    "\n",
    "isDupplicate = duplicateLine(linesCode)\n",
    "print(isDupplicate)\n",
    "\n",
    "#close file\n",
    "file.close()\n",
    "\n",
    "elapsed_time = time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
