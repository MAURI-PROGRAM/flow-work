{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_onlyCode(allLines):\n",
    "    copyline = allLines.copy()\n",
    "    newLines=[]\n",
    "    lines=iter(allLines)\n",
    "    citer=0\n",
    "    withoutSpace = lambda x : x.replace(' ','')\n",
    "    blankLine = lambda x : x=='\\n' or x==''\n",
    "    commWildcard = lambda x:x[0]=='#'\n",
    "    commOneline = lambda x:(initComment(x) and endComment(x[3:]))\n",
    "    initComment = lambda x:(x[:3]=='\"\"\"')\n",
    "    endComment = lambda x:(x[-4:-1]=='\"\"\"')\n",
    "    for line in lines:\n",
    "        citer=citer+1\n",
    "        newLine = withoutSpace(line)\n",
    "        if blankLine(newLine):\n",
    "            continue\n",
    "        if commWildcard(newLine):\n",
    "            continue\n",
    "        if commOneline(newLine):\n",
    "            continue\n",
    "        if initComment(newLine):\n",
    "            try:\n",
    "                line = next(lines)\n",
    "            except:\n",
    "                pass\n",
    "            for i in range(citer,len(copyline)):\n",
    "                newLine = withoutSpace(line)\n",
    "                endLine = citer+1==len(copyline)\n",
    "                try:\n",
    "                    line = next(lines)\n",
    "                except:\n",
    "                    pass\n",
    "                if endComment(newLine) or endLine:\n",
    "                    break\n",
    "                else:\n",
    "                    citer=citer+1\n",
    "        newLines.append(line)   \n",
    "    return newLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depFor(archivo):\n",
    "    newLineas=archivo.copy()\n",
    "    lineas=iter(archivo)\n",
    "    citer=0\n",
    "    withoutSpace = lambda x : x.replace(' ','')\n",
    "    initFor = lambda x:(x[:3]=='for')\n",
    "    endFor = lambda x:(x[-2:-1]==':')\n",
    "    countSpace = lambda x:(x.index(x.replace(' ','')[0]))\n",
    "    arrMax=[0]\n",
    "    arr=[]\n",
    "    for linea in lineas:\n",
    "        citer=citer+1\n",
    "        wordFor='for'\n",
    "        wordEnd=':'\n",
    "        space = countSpace(linea)\n",
    "        patronFor=((initFor(withoutSpace(linea))) and (endFor(withoutSpace(linea))))\n",
    "        if patronFor:\n",
    "            newLines=[]\n",
    "            for i in range(citer,len(newLineas)):\n",
    "                endLine = citer+1==len(newLineas)\n",
    "                forSpace = False\n",
    "                if not endLine:\n",
    "                    nextspaceLine = countSpace(newLineas[citer])\n",
    "                    forSpace = nextspaceLine<=space\n",
    "                else:\n",
    "                    try:\n",
    "                        newLines.append(next(lineas))\n",
    "                    except:\n",
    "                        pass\n",
    "                if forSpace or endLine:\n",
    "                    value,arr=depFor(newLines)\n",
    "                    arrMax.append(value+1)\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        newLines.append(next(lineas))\n",
    "                        citer=citer+1\n",
    "                    except:\n",
    "                        pass\n",
    "    return(max(arrMax),arrMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def librerias(lines):\n",
    "    setlibrerias=set()\n",
    "    wordImport= 'import'\n",
    "    wordFrom= 'from'\n",
    "    notElement={'import', 'from', ''}\n",
    "    for line in lines:\n",
    "        inImport=(wordImport in line)\n",
    "        inFrom=(wordFrom in line)\n",
    "        if inFrom and inImport:\n",
    "            arrlibr = line[:line.index('import')].split(' ')\n",
    "            setlibrerias = setlibrerias|set(arrlibr)\n",
    "            continue\n",
    "        if inImport:\n",
    "            arrlibr =  line[:-1].split(' ')\n",
    "            setlibrerias = setlibrerias|set(arrlibr)\n",
    "            continue\n",
    "    newSetelement= list(setlibrerias-notElement)\n",
    "    newArrelement = [ element.split('.')[0] for element in newSetelement]\n",
    "    return set(newArrelement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDef(lines):\n",
    "    wordDef='def'\n",
    "    wordEnd=':'\n",
    "    arrArg=[]\n",
    "    for line in lines:\n",
    "        if wordDef in line and wordEnd in line:\n",
    "            argini=line.find(\"(\")+1\n",
    "            argend=line.find(\")\")\n",
    "            arg=line[argini:argend].split(',')\n",
    "            arrArg.append(len(arg))\n",
    "    return arrArg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicateLine(arrlines):\n",
    "    newGroup=[]\n",
    "    withoutSpace = lambda x : x.replace(' ','')\n",
    "    lines=list(map(withoutSpace,arrlines))\n",
    "    unionLine=lambda x : x.replace('[','').replace(',','').replace(']','')\n",
    "    for i,line in enumerate(lines):\n",
    "        newGroup.append(str(unionLine(str(lines[i:i+4]))))\n",
    "    repetidos=collections.Counter(newGroup)\n",
    "    duplicado = [1 for key in repetidos if repetidos[key]>=2]\n",
    "    return (sum(duplicado)+3*bool(duplicado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnFilespy(carpeta):\n",
    "    newArray=[]\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        if os.path.isdir(os.path.join(carpeta,archivo)):\n",
    "            newArray = newArray+returnFilespy(os.path.join(carpeta,archivo))\n",
    "        else:\n",
    "            newArray.append(os.path.join(carpeta,archivo))\n",
    "    filesPy = [filePy for filePy in newArray if filePy[-3:]=='.py' ]\n",
    "    return filesPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3411"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruoteFlies = '.\\\\Almacen_repositorios\\\\tensorflow-master'\n",
    "filesPy = returnFilespy(ruoteFlies)\n",
    "len(filesPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________\n",
      "\n",
      "number of lines : 696194\n",
      "libraries : ['', 'mark_as_used=False)', '_contextlib', '3]))', '_import_and_infer(', 'npr', 'submodules', 'new_value', 'test_op', 'TF-TRT', '4', 'prob', 'If', \"help='Base\", 'disable=wildcard-import', 'na_values=\"?\")', 'import_event(tensor,', 'np', 'third_path,', 'Fs(20),', '_init_from_proto(self,', 'summary_ops_v2', 'restored_fullargspec', 'has', 'new_saver_3', 'tf_saver', 'output_op', 'symbol_id', 'open(self', '3]),', '%', 'tempfile', '\"b\"]),', 'op_index,', \"'problem\", '3', 'functools', 'hashlib', 'imported({\"a\":', 'inspect_utils', 'name=\"\")', 'psutil', 'expected_', 'special', 'b_1', 'api_name):', '\\\\n\")', 'Necessary', 'traceback', 'Do', '[\"tag_name\"],', 'site;', 'saver_lib', 'input_layer', 'import_scope=\"scope_name\")', '!=', '_init_values_from_proto(self,', 'imported_concrete,', '\"Feature', 'import_scope=\"bar\")', 'summary', 'import_scope=\"new_hidden1/new_hidden2\",', 'import_scope=\"subgraph_1\")', 'g', 'marshal', \"name='')\", 'input_x,', 'output,', 'import_scope=to_scope)', 'tarfile', 'test_structure_import(self,', 'run_init_ops(self,', 'to', '(k,', 'warnings', 'import_scope', 'try_import(name):', 'from_proto(context_def,', 'gen_summary_ops', '[loss()', 'target', 'result', 'math', 'imported_tensor', 'setuptools', 'setattr', 'will', 'from_proto(queue_runner_def,', 'docker', 'sre_constants', '_get_feature_importances(', '_get_op_from_signature_def(meta_graph_def,', '_create_saver_from_', 'typing', '`Tensor`', 'io', '__init__(self,', 'import_scope=\"baz\")', 'list(imported', 'c', 'urllib', 'avoid', 'load(self,', '_numbers', '**saver_kwargs)', '(expected_import,', '1', 'imports', 'name=(import_scope', 'tf_export', '(name,', 'zipfile', 'comment', 'imports,', 'inp,', 'output_layer', '_import_graph_def(graph_def,', 'idx,', 'saver,', '\"import/\"', 'sf', '360', 'because', 'collections_lib', 'imported', '_json', \"'import_graph_def')\", 'tag=\"feature_importance/usage_counts\",', 'concurrent', 'tensorflow', 'compat', 'attr', 'types_lib', 'names', 'equal', '__builtin__', 'python', 'be', 'nets', '2,', '[\"gamma\",', 'There', 'load_op_from_signature_def(signature_def,', 'wrapped_import', 'py_collections', 'module_text_map[dest_module]', 'b', 'variable_scope', 'cpuinfo', 'r\"/\\\\2\",', 'curses', 'variable_def,', 'test_getfutureimports_lambdas(self):', \"'from\", '%s:', '7', 'meta_graph', 'import_scope[-1]', 'pydotplus', 'import_scope=scope_to_prepend_to_names))', 'var_v1', 'tensorflow_docs', 'sampling', 'epochs=3)', 'loss', 'Failed', '@test', 'expected', 'imported_concrete', 'this', 'test_dir', 'classifier_outputs', 'node', 'enumerate(self', 'please', 'variable_def', 'of', 's:', 'init_op', 'keras', 'json', 'absolute_import', 'part', 'builtins', 'print(\"****', 'v)', 'import_graph_def(graph_def,', 'imported_output)', \"'-c',\", 'dd', '}\",', '16', 'import_scope=\"importA\")', 'learning_rate=0', 'imported(constant_op', \"\\\\'import/b\\\\'\", 'signatures={})', 'pd', '\\\\\\'_tpu_replicate\\\\\\'\"):', 'epochs=4)', '{', 'imported_vars)', \"'global_feature_importance'\", 'try_import(\"scipy', 'You', 'ctypes', 'Fs(10))\"', 'sorted(', \"'tf\", 'import_scope=import_scope)', 'import', '\"alpha\"]}))', 'free', 'log_e_x2', 'sklearn', 'module_code_builder', 'omit_collection_keys):', \"'TestClass'\", 'disable=g-import-not-at-top,unused-variable', 'modules', 'soundfile', 'sure', 'names=imports85', '(lambda:', 're', \"feature_importances']\", '**kwargs):', '{\"keys\":', 'cycles)', \"msg='%s\", 'named', 'getfutureimports(entity):', 'important', \"'python/platform/control_imports\", 'math_ops', 'fftpack', 'import_scope[:-1]', 'import_scope=\"my_scope\")', '{\"key1\":', 'tf', 'dest_name):', 'grpc', 'before', 'dest_name_to_imports', '(train,', '_imports_graph_def():', 'set(imported', 'wrapt', 'new_saver', 'custom_export_strategy', 'dependency', 'python_types', 'loader', 'nest', '+', 'SavedModel', \"'If\", \"pydot'\", 'context_def,', \"frozenset(['absolute_import',\", 'tfp', 'excess', 'absl', '_collections', 'queue', 'library:', 'topo_order', 'second_path,', 'dnn_regression', 'constant_op', 'report_feature_importances=report_feature_importances,', 'path', 'output_op_name', 'further', 'Need', 'string', 'save', 'imported_function(two)[\"output_0\"]', 'imported_function', 'alias', 'that', 'itertools', \"name='import')\", 'critical_section_def=None,', '_os', 'from_proto(variable_def,', 'disable=unused-import,g-bad-import-order,g-import-not-at-top,wildcard-import', 't,', 'imported_modules:', 'str(e)))', '\"meta_graph', 'import_scope):', 'you\")', '_get_feature_importances(dtec,', '\"https://archive', 'data', '==', 'var_list', 'and', 'set,', 'accidental', 'output', 'first_imported', 'output1', 'was', 'Summary', 'plt', 'wrap_function(_imports_graph_def,', '_import_and_infer(save_dir,', 'ValueError(\"Exporting/importing', 'pasta', '_import_meta_graph(self):', '{\"output_0\":', 'h5py', 'import_and_eval(flags_obj):', 'import_scope=\"import\")', 'import_scope=import_scope,', 'args', 'random', 'import_scope=\"\")', '_test_import(include_collection_keys,', \"('absolute_import',\", 'can', '_test_import(', 'tf\\\\na', 'proto', \"\\\\'import/y\\\\'\", 'tensor_name)', 'disable=,wildcard-import,unused-import', \"'import\", 'psutil_import_succeeded:', 'original_graph_def,', 'toolz', 'FLAGS', 'd', 'importing', 'imported_vars[sample_key]', 'linecache', 'defg=7', 'imported_constant', \"msg='compat\", 'fnmatch', 'True', '[b\"loc:@imported_graph/A\"])', ':)', 'Please', 'cycles):', 'collections', 'into', 'import:', \"''\", 'scipy_sparse', 'in', 'by', 'import_meta_graph(meta_graph_or_file,', '\"\",', '_error', 'scope=import_scope)', 'you,', 'parent_pkg', 'tf;\\\\', 'places=4)', '_re', 'return_elements=[\"B\"],', 'their', 'readline', 'pipes', 'g_concrete', 'FUTURES_PATTERN', 'output_name', 'key=lambda', 'an', 'v', 'tfe', 'dst_graph=copied_graph,', 'test_load_with_import_scope(self,', 'saver_module', 'dest_name)', 'function', 'datetime', 'c,', 'get_train_op(meta_graph_def,', 'tflite_runtime', 'expressions', 'code', 'WhileContext(context_def=context_def,', 'len(self', 'trained', 'fractions', '`input_map`', 'as', 'imp', '\"important', 'load(sess,', 'atexit', '[2', 'spec', 'raise', '-1,', '{}', 'six', 'tape', 'ct', 'weakref', 'tag=\"feature_importance/gains_fraction\",', 'distribute_strategy=distribute_strategy,', '*', '1,', '_os_path', 'e_x', 'non-empty', '4]))', 'format_import(source_module_name,', 'clear_devices=True,', '_compute_gradient(imported', 'print_function', 'self', 'queue_runner_def', 'issue', 'from_control_flow_context_def(nested_def,', '_original_from_proto(v,', '-1],', 'meta', 'feature', '[b\"loc:@imported_graph/B\"])', \"ImportError('Failed\", '=', 'graph_builder', 'import_scope=\"s\")', 'gast', 'get_init_op(meta_graph_def,', '_import_submodules(self):', '_import_and_infer(second_dir,', 'imported_function(x=two)[\"output_0\"]', 'FUTURES_PATTERN_3', 'dask', 'queue_runner_def=None,', 'imported_path', 'kubernetes', 'during', \"r'^from\", '\"beta\"]}))', '\"%s/%s\"', 'timeit', '2],', 'calling', 'stats', 'import_scope=\"new_hidden2\",', 'gen_lookup_ops', 'mock', 'input_name', \"'does\", 'API', 'applied', 'feature_importances[\"feature_a_0\"],', 'IPython', 'logging', 'sparsemax_loss', 'new_saver_1', '\"\"\"from', 'a,', 'train', 'new_var_list', 'third_import', 'input_map,', 'open(imported_output,', 'importance', 'errno', 'unittest', 'import_scope=\"baa\")', 'the', 'keras_applications', 'from_proto(hparam_def,', 'scope:', 'Queue', \"name='expectation_importance_sampler'):\", 'imported_modules', '[\"foo_graph\"],', '[0', '_,', 'str(imports)))', 'list(imported_vars', 'k,', 'pylint:disable=g-import-not-at-top', 'class_node),', 'disable=raising-bad-type', 'list(tape', 'numpy', 'tf_logging', 'future_import_module_statements', 'Unused', 'queue_runner_def,', '[b\"loc:@imported_graph/A\",', 'import_scope=None,', 'PIL', 'IMPORTS', 'e_x2', 'return', 'google', 'dst_scope=\"imported\")', 'sess', 'FEATURE_IMPORTANCE_NAME', 'k', '(', 'test)', 'pydot_ng', 'scope', 'proto,', 'The', 'astor', 'run', 'sorted(imports)[0]', 'get_element_from_tensor_info(tensor_info,', 'f', 'here', '\"Operation', 'memory_profiler', 'parent_module', 'portpicker', \"'expectation_importance_sampler',\", '{\"_get_imports85\":', 'csv', 'yaml', 'import_scope,', '`Log[f]`', 'binary', 'meta_graph_def_to_load,', 'pylint:', '[op', 'getpass', 'x', 'parent_module,', 'graph', 'matplotlib', 'uuid', '_platform', 'urllib2', '_six', 'run_shell([python_bin_path,', '`expectation_importance_sampler_logspace`', 'text', 'estimate', '{model', 'sgdr_learning_rate_decay', 'tensorflow_probability', 'import_to_tensorboard(FLAGS', 'full_api_name)', 'feature_importances(self):', '[])', '-x[1])', 'critical_section_def,', 'stat', 'import_scope=None):', 'var_list,', '(source_name,', 'name=import_scope)', 'mc', 'f_concrete', 'import_scope:', 'item', 'binascii', \"'print_function'])\", 'sys', 'imported_graph', 'sys;', 'constants', 'graphs', 'gc', 'root', 'logits_out', 'outputs))', 'import_scope=\"new_queue1\")', 'test_resource_variable_import(self):', 'it', 'oauth2client', 'must', '(import_scope,', 'saver', 'threading', 'test_getfutureimports_methods(self):', 'tiled_imported,', 'zlib', 'E', '\"some_scope_name\"', \"ops_at_end['1:\", '(expected,', 'pprint', 'from_control_flow_context_def(context_def,', 'import_scope=\"subgraph_2\")', 'future_imports', 'termcolor', 'cStringIO', 'requests', 'export_scope=\"import\")', '@tf_export(v1=[\"train', 'variables', 'num_dense_floats,', 'fn', '_inspect', 'ops', 'codecs', 'argparse', 'expectation_importance_sampler(f,', 'Conc(1,', 'boto3', 'scope_to_prepend_to_names),', 'kwargs', '2', 'tensor_info,', 'print(\"\\\\\\\\n\"', 'gzip', 'imported(NamedTupleType(a=constant_op', '%f\"', 'os', 'feature_importances', 'cycles=1,', 'dest_module,', 'public', 'signal', '_subprocess', 'frozenset(important_op_names)', 'inputs):', 'None)', 'run_context', '#', 'add_imports_for_symbol(', '%s', '_np', 'sqlite3', 'pydot', 'clear_devices=False)', 'difflib', 'import_scope=import_scope))', '(api_name', 'import_graph', 'imported_vars,', 'from_proto(proto,', 'site', 'import_scope)', 'op_hint', 'imported_output', '\"b\":', 'imported_return_elements', 'operator', 'import_scope))', 'with', 'inspect', 'saver_for_restore', \"test_op')\", 'time', '`name`', 'base_tensor_name', \"_TEST_CONSTANT')\", 'conversion', 'imported\")', 'keras_preprocessing', 'import_scoped_meta_graph_with_return_elements(', '[[3', 'f:', 'imported_graph:', 'tensorboard', 'is', 'signatures=imported', 'values_def=None,', 'imports85', '\"/\":', 'needs', \"TestClass')\", 'grad', 'necessary', 'import_scope=\"new_hidden1\")', 'restorer', 'install', 'pdb', 'graph=graph2)', 'disable=g-bad-import-order', 'platform', 'test_getfutureimports_functions(self):', 'ret', 'feature_importances[\"feature_d\"],', 'struct', 'feature_importances[\"feature_b\"],', 'Unable', '_import_and_infer(export_dir,', 'socket', 'REQUIRED_FUTURES', 'base64', 'import_scope=\"new_softmax_linear\",', 'deal', \"'Model')\", '-c', 'single_image_random_dot_stereograms', 'context_def', 'disable=g-import-not-at-top,unused-import', 'len(imported', 'concrete', 'log_dir):', 'or', 'simple_value=gain)', '_site', '_copy', 'r\"\\\\1\"', 'tensor_name', 'def', 'abc', 'net', 'imported_variables', 'import_scope=\"test_scope\")', 'set(self', 'second_import', '_TestDir(\"selected_collections_import\"),', '__future__', '\"v:0\")', '[]', 'requires', 'fcntl', 'mpi', 'filename', '_tempfile', '{\"x\":', '_time', \"test_op1')\", 'export_dir,', '(import_node,', '\"moment', 'textwrap', 'used', 'input_map', 'str(imports', '[', 'orig_meta_graph,', 'import_scope=\"foo\")', \"%s'\", 'text,', 'types', 'for', 'we', 'values_def=context_def', 'This', '_get_imports85()', 'linear_regression_categorical', 'op', 'ssl', '\"tf', 'expected_import', 'math_ops;', 'values_def=c', 'if', 'shlex', 'return_elements=[7])', 'ast', 'newgraph', '\"w\")', 'importer', 'import_scope=\"new_hidden1\",', 'restore_variables(self,', 'test_ref_variable_import(self):', 'error', 'signatures=second_import', 'googleapiclient', 'RuntimeError(\"Exporting/importing', 'sorted_by_importance', 'tensorflow_estimator', 'save_slice_info_def', 're;', 'out', 'None)})', '_get_imports85():', 'make', 'var_names', 'importlib', 'disable=invalid-name', 'file', 'import_scoped_meta_graph(orig_meta_graph,', 'add_import(', '_logging', 'input_y', 'future_features', 'distutils', 'sess,', 'disable=protected-access', 'fetches', '_import_config(self):', \"import_scope='new_outer')\", 'inconsistent', 'x:', 'Avoids', '\"Feel', \"print('Failed\", 'NotImplementedError(\"variable_scope', '%s\"', 'disable=unused-import', 'colorsys', 'b\"loc:@imported_graph/B\"])', 'do', 'second_imported', '0,', 'meta_graph_def,', 'not', 'a_1,', \"'expectation_importance_sampler_logspace',\", 'report_feature_importances:', 'since', 'expected_shape=expected_shape,', 'copy', 'pandas', 'inputs),', '_traceback', 'TensorBoard', 'get_asset_tensors(export_dir,', 'load_graph(self,', 'subprocess', 'source_name', 'op_signature_key,', '{}))', 'True)', 'name', 'no', 'pycoll', '\"import', 'load', 't', '_portpicker_import_error', 'output2', 'packages', 'import_str', '\"y\":', 'sample_var', 'app', 'have', 'child_pkg', 'CondContext(context_def=context_def,', 'imported_constant,', 'tags,', \"'import',\", 'contextlib', 'new', '5))}))', '**saver_kwargs):', 'becoming', \"'\\\\n'\", 'supplement', 'tag=\"feature_importance/usage_fraction\",', 'hook', 'clear_devices=False,', 'get_tensor_from_tensor_info(tensor_info,', 'import_scope=scope_to_prepend_to_names)', 'PLACEHOLDER\"', 'new_saver_2', 'module', 'scipy', 'package', '_check_already_imported(self,', 'containing', 'imported_vars):', '[imported', 'glob', '_TestDir(\"scoped_export_import\")', 'values', 'report_feature_importances=False,', '1)', '_from_proto_fn(v,', \"'`input_map`\", 'dtype=imports85', '_sys', 'URL', 'disable=g-import-not-at-top', 'circular', '_math', 'str(imports),', '_', \"'division',\", '\"list', 'cycles,', 'name=\"imported_graph\")', 'important_op_names', 'measures', 'print(sys', 'meta_graph_or_file,', \"'\", 'b,', 'supported', 'False', 'variable_def=variable_def,', 'shutil', 'values_def,', 'key,', 'from_proto(saver_def,', 'a', 'clear_devices=True)', '\\'\"#API', 'important_op_names:', 'builder_cls):', 'psutil_import_succeeded', \"set(['absolute_import',\", 'lengths', '{})[\"output_0\"]', 'enum', 'z3', 'feature_names,', '`shape`', '_uuid', 'ResourceVariable', 'replaced', 'prepend_name_scope(name,', 'variadic', 'symbol_id,', \"@tf_export('graph_util\", 'pickle', 'import_scope=\"new_model\")', 'name,', 'name=\"final\")', 'numbers', 'sorted_by_importance))', 'import_scoped_meta_graph(meta_graph_or_file,', 'array_ops', 'cgi', 'tf_inspect', 'None', 'clear_devices,', 'tokenize', '3,', '_distutils', 'graph=graph,', 'gym', 'exist', 'graph=graph_2,', 'feature_importances[\"feature_a_m_3\"],', 'graph=g,', 'source_name,', 'sps', 'graph,', 'import_to_tensorboard(model_dir,', 'future_import_module_statements)', 'del', \"'loc:@imported_graph/A'\", 'simple_value=usage_count)', '\"r\")', 'cPickle', '_importer', 'Saver(saver_def=saver_def,', 'multiprocessing', 'imports_list', 'ImportError(\"Could', 'tiled_imported', 'graph=None,', 'cerberus', 'name=None):', '\"', 'Tensorflow', 'when', 'create_python_api', 'custom_regression', 'third_party', '_portpicker_import_error:']\n",
      "nesting factor : 1.2306426976853484\n",
      "code duplication : 5.609643289083215\n",
      "average variables : 1.5371785700839578\n",
      "9.026854753494263\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "LinesCodetotal = 0\n",
    "depForTotal = []\n",
    "argDeftotal = []\n",
    "libreryTotal =set()\n",
    "duplicateTotal = 0\n",
    "for filePy in filesPy:\n",
    "#     print('********************************************************')\n",
    "#     print(filePy)\n",
    "    file = open(filePy, 'r', encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    linesCode = def_onlyCode(lines)\n",
    "    LinesCodetotal=LinesCodetotal+len(linesCode)\n",
    "#     print(len(linesCode))\n",
    "    \n",
    "    countFor = depFor(linesCode)[1][1:]\n",
    "    depForTotal = depForTotal+countFor\n",
    "#     print(countFor)\n",
    "    \n",
    "    libreriasSet = librerias(linesCode)\n",
    "    libreryTotal = libreryTotal|libreriasSet\n",
    "#     print(libreriasSet)\n",
    "    \n",
    "    countArgdef = countDef(linesCode)\n",
    "    argDeftotal = argDeftotal+countArgdef\n",
    "#     print(countArgdef)\n",
    "    \n",
    "    isDupplicate = duplicateLine(linesCode)\n",
    "    duplicateTotal=duplicateTotal+isDupplicate\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "avgFor=sum(depForTotal)/len(depForTotal)\n",
    "perctDuplicate = duplicateTotal/LinesCodetotal*100\n",
    "avgArg = sum(argDeftotal)/len(argDeftotal)\n",
    "\n",
    "print('____________________________________________________')\n",
    "print()\n",
    "print('number of lines : ' +str(LinesCodetotal))\n",
    "print('libraries : '+str(list(libreryTotal)))\n",
    "print('nesting factor : '+str(avgFor))\n",
    "print('code duplication : '+str(perctDuplicate))\n",
    "print('average variables : '+str(avgArg))\n",
    "\n",
    "elapsed_time = time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047\n",
      "[1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
      "{\"'-c',\", 'os', 'sys;', 'run_shell([python_bin_path,', '__future__', 're', 'argparse', 'distutils', 'platform', \"'import\", 'print(sys', 'site;', \"'from\", 'print(\"\\\\\\\\n\"', 'subprocess', 'shutil', 'errno', 'sys', 'return'}\n",
      "[1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n",
      "8\n",
      "0.015962600708007812\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "# for filePy in filesPy\n",
    "#open file\n",
    "file = open(r'.\\Almacen_repositorios\\tensorflow-master\\configure.py', 'r')\n",
    "lines = file.readlines()\n",
    "\n",
    "##Prosesing file\n",
    "\n",
    "#Lines of code\n",
    "linesCode = def_onlyCode(lines)\n",
    "# for line in linesCode:\n",
    "#     print(line)\n",
    "print(len(linesCode))\n",
    "\n",
    "countFor = depFor(linesCode)\n",
    "print(countFor[1][1:])\n",
    "\n",
    "libreriasSet = librerias(linesCode)\n",
    "print(libreriasSet)\n",
    "\n",
    "countArgdef = countDef(linesCode)\n",
    "print(countArgdef)\n",
    "\n",
    "isDupplicate = duplicateLine(linesCode)\n",
    "print(isDupplicate)\n",
    "\n",
    "#close file\n",
    "file.close()\n",
    "\n",
    "elapsed_time = time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
